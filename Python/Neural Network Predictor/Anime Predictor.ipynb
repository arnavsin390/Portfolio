{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "D76DyCDhgfx3",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "outputId": "11ee272d-e213-4dd4-db57-dc5efe314079"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 44 unique genres: action, adventure, cars, comedy, dementia, demons, drama, ecchi, fantasy, game, harem, hentai, historical, horror, josei, kids, magic, martial arts, mecha, military, music, mystery, parody, police, psychological, romance, samurai, school, sci-fi, seinen, shoujo, shoujo ai, shoujo\", shounen, shounen ai, slice of life, space, sports, super power, supernatural, thriller, vampire, yaoi, yuri\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "#Input file is animes.xlsx\n",
    "fname = os.path.join('..', 'resource', 'asnlib', 'publicdata', 'animes.xlsx')\n",
    "\n",
    "# Getting all the unique genres present in the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file with the specified sheet name\n",
    "df = pd.read_excel(fname, sheet_name=\"Extracted\", engine='openpyxl')\n",
    "\n",
    "# Concatenate all entries in the column and split by comma\n",
    "all_words = ','.join(df['genres']).split(',')\n",
    "# Remove leading/trailing whitespaces and convert to lowercase\n",
    "all_words_cleaned = [word.strip().lower() for word in all_words]\n",
    "# Get unique words\n",
    "unique_words = set(all_words_cleaned)\n",
    "# Print unique words\n",
    "print(f'Found {len(unique_words)} unique genres: {\", \".join(sorted(unique_words))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "hPRyTneag5up",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "outputId": "0c899bc8-6154-4462-9665-ff7b5b085f85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most occurring words:\n",
      "comedy: 1202\n",
      "action: 995\n",
      "fantasy: 778\n",
      "slice of life: 550\n",
      "school: 545\n",
      "music: 507\n",
      "drama: 492\n",
      "supernatural: 437\n",
      "adventure: 428\n",
      "romance: 421\n"
     ]
    }
   ],
   "source": [
    "# Getting the top 10 genres present in the data\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_counts = Counter(all_words_cleaned)\n",
    "# Find top 10 most occurring words\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "# Print top 10 most occurring words\n",
    "print(\"Top 10 most occurring words:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "_dC7DfIhiEWL",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3051 of 3479 remaining\n"
     ]
    }
   ],
   "source": [
    "# Creating a filtered dataset in which we only have the rows that have only those top 10 genres\n",
    "\n",
    "# Find top 10 most occurring words\n",
    "top_10_words = [word for word, _ in word_counts.most_common(10)]\n",
    "\n",
    "# Function to filter rows based on top 10 words\n",
    "def filter_rows(row):\n",
    "    words_in_row = [word.strip().lower() for word in row['genres'].split(',')]\n",
    "    return any(word in top_10_words for word in words_in_row)\n",
    "\n",
    "# Filter rows\n",
    "filtered_df = df[df.apply(filter_rows, axis=1)].copy()\n",
    "\n",
    "# Remove other columns\n",
    "#filtered_df = filtered_df[['genres']]  # Adjust column name as needed\n",
    "\n",
    "# Remove other words from entries\n",
    "filtered_df['genres'] = filtered_df['genres'].apply(lambda x: ','.join(word for word in x.split(',') if word.strip().lower() in top_10_words))\n",
    "\n",
    "# Reset index\n",
    "filtered_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(f'{len(filtered_df)} of {len(df)} remaining')\n",
    "\n",
    "#filtered_df.to_excel(\"anime_filtered_data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wWUTORve3wpq",
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3051 of 3479 remaining\n"
     ]
    }
   ],
   "source": [
    "# Creating a one hot encoding for the genres column and generating that data file\n",
    "\n",
    "# Concatenate all entries in the column and split by comma\n",
    "all_words = ','.join(filtered_df['genres']).split(',')\n",
    "\n",
    "# Remove leading/trailing whitespaces and convert to lowercase\n",
    "all_words_cleaned = [word.strip().lower() for word in all_words]\n",
    "\n",
    "# Get unique words\n",
    "unique_words = set(all_words_cleaned)\n",
    "\n",
    "# Count occurrences of each word\n",
    "word_counts = Counter(all_words_cleaned)\n",
    "\n",
    "# Find top 10 most occurring words\n",
    "top_10_words = [word for word, _ in word_counts.most_common(10)]\n",
    "\n",
    "# Function to filter rows based on top 10 words\n",
    "def filter_rows(row):\n",
    "    words_in_row = [word.strip().lower() for word in row['genres'].split(',')]\n",
    "    return any(word in top_10_words for word in words_in_row)\n",
    "\n",
    "# Filter rows based on the presence of top 10 genres and operate on a copy to avoid SettingWithCopyWarning\n",
    "filtered_df = filtered_df[filtered_df.apply(filter_rows, axis=1)].copy()\n",
    "\n",
    "# Now we can safely apply transformations to 'genres' without causing SettingWithCopyWarning\n",
    "filtered_df['genres'] = filtered_df['genres'].apply(lambda x: ','.join(word for word in x.split(',') if word.strip().lower() in top_10_words))\n",
    "\n",
    "# One-hot encode the column with top 10 words\n",
    "one_hot_encoded_df = filtered_df['genres'].str.get_dummies(sep=',')\n",
    "\n",
    "# Reset index\n",
    "one_hot_encoded_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the one-hot encoded DataFrame\n",
    "#print(one_hot_encoded_df)\n",
    "\n",
    "# Combine one-hot encoded columns with original DataFrame\n",
    "combined_df = pd.concat([filtered_df, one_hot_encoded_df], axis=1)\n",
    "print(f'{len(combined_df)} of {len(df)} remaining')\n",
    "#combined_df.to_excel(\"filtered_data.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = combined_df\n",
    "\n",
    "# Convert 'start_date' to a datetime, if it's not already\n",
    "df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "\n",
    "# Extract numerical features from 'start_date'\n",
    "df['year'] = df['start_date'].dt.year\n",
    "df['month'] = df['start_date'].dt.month\n",
    "df['day'] = df['start_date'].dt.day\n",
    "# Drop the original 'start_date' column\n",
    "df = df.drop('start_date', axis=1)\n",
    "\n",
    "# Select features and target \n",
    "X = df[['media', 'episodes', 'members', 'year', 'month',\n",
    "        'source', 'season_cleaned', 'Action', 'Adventure', 'Comedy', 'Drama',\n",
    "        'Fantasy', 'Music', 'Romance', 'School', 'Slice of Life', 'Supernatural']]\n",
    "y = df['rating']\n",
    "\n",
    "# Convert categorical columns to numeric using One-Hot Encoding if not already done\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uc50uaMcDeOs",
    "outputId": "2b859987-abcb-4ba7-9b17-13441a386088"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "def simple_nn():\n",
    "    return MLPRegressor(hidden_layer_sizes=(10,), max_iter=200, random_state=42)\n",
    "\n",
    "# You can either replace the RHS below with your own function, or define a function \"solution_nn\" directly\n",
    "#solution_nn = simple_nn\n",
    "\n",
    "###\n",
    "### YOUR CODE HERE\n",
    "###\n",
    "def solution_nn():\n",
    "    \n",
    "   \n",
    "    genres_df = df['genres'].str.get_dummies(sep=',')\n",
    "    \n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['source_encoded'] = le.fit_transform(df['source'])\n",
    "    \n",
    "   \n",
    "    X = pd.concat([genres_df, df[['members', 'source_encoded']]], axis=1)\n",
    "    y = df['rating']\n",
    "    \n",
    "    \n",
    "    model = MLPRegressor(hidden_layer_sizes=(50, 50), activation='logistic', alpha=0.0001, \n",
    "                         learning_rate='adaptive', random_state=42)\n",
    "    \n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"MSE:\", mse)\n",
    "    \n",
    "    return pipeline.named_steps['regressor']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ph_NRUbcZjC4",
    "nbgrader": {
     "grade": true,
     "grade_id": "No Errors",
     "locked": true,
     "points": "50",
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.47626766757925215\n",
      "Model trained in 6 seconds\n",
      "MSE=0.39867133402827115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "mlp = solution_nn()\n",
    "assert isinstance(mlp, MLPRegressor), 'Solution must be an MLPRegressor'\n",
    "start = time.time()\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "duration = time.time()-start\n",
    "print(f'Model trained in {int(duration)} seconds')\n",
    "predictions = mlp.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'MSE={mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Great MSE",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error was low enough for full credit on accuracy\n"
     ]
    }
   ],
   "source": [
    "assert mse < 0.4, 'The network\\'s error was too high for full credit'\n",
    "print('The error was low enough for full credit on accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Good MSE",
     "locked": true,
     "points": "10",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error was low enough for partial credit\n"
     ]
    }
   ],
   "source": [
    "assert mse < 0.5, 'The network\\'s error was too high for partial credit'\n",
    "print('The error was low enough for partial credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "Fast",
     "locked": true,
     "points": "20",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your MLP receives full credit for accuracy and speed\n"
     ]
    }
   ],
   "source": [
    "# Testing that you can achieve high accuracy in a short amount of time\n",
    "assert duration < 30, 'Your MLP training took too long'\n",
    "assert mse < 0.4, 'Ineligible for points for speed due to low accuracy'\n",
    "print('Your MLP receives full credit for accuracy and speed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
